{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:42.436311Z",
     "start_time": "2023-04-20T21:30:31.624531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (1.22.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.24.2 which is incompatible.\n",
      "scipy 1.7.0 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.2 which is incompatible.\n",
      "intel-tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.24.2 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.24.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:53.494195Z",
     "start_time": "2023-04-20T21:30:42.438897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.7.0)\n",
      "Collecting numpy>=1.18.5\n",
      "  Using cached numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.22.4 which is incompatible.\n",
      "intel-tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.22.4 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.22.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:54.546676Z",
     "start_time": "2023-04-20T21:30:53.497005Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:54.554621Z",
     "start_time": "2023-04-20T21:30:54.548927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.22.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:54.560600Z",
     "start_time": "2023-04-20T21:30:54.556352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:57.519598Z",
     "start_time": "2023-04-20T21:30:54.562407Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import nltk,string\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import SoftCosineSimilarity, Similarity\n",
    "\n",
    "import pickle\n",
    "import scipy\n",
    "from typing import Tuple\n",
    "\n",
    "from gensim.similarities import SparseTermSimilarityMatrix, SoftCosineSimilarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:57.559460Z",
     "start_time": "2023-04-20T21:30:57.521917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:57.570015Z",
     "start_time": "2023-04-20T21:30:57.562462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(\"termisim_ups.pkl\", \"rb\") as f:\\n    termsim_matrix = pickle.load(f)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables to use \n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "HARD_COS_WEIGHT = 0.3\n",
    "SOFT_COS_WEIGHT = 0.7\n",
    "\n",
    "KEY_COLUMNS = ['client_property_name', 'workorder_number', 'workorder_desc',\n",
    "               'workorder_creation_date_ts', 'SLA', 'priority_code', 'type_code']\n",
    "'''with open(\"termisim_ups.pkl\", \"rb\") as f:\n",
    "    termsim_matrix = pickle.load(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:57.625362Z",
     "start_time": "2023-04-20T21:30:57.572192Z"
    }
   },
   "outputs": [],
   "source": [
    "workorders = pd.read_csv(\"DATA/SCB_IN_SG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:57.631352Z",
     "start_time": "2023-04-20T21:30:57.627272Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(workorders: pd.DataFrame, splitter: str) -> pd.DataFrame:\n",
    "    \n",
    "    date_cols = [cols for cols in workorders.columns if \"_ts\" in cols]\n",
    "    workorders[date_cols] = workorders[date_cols].apply(pd.to_datetime)\n",
    "        \n",
    "    mask = workorders['workorder_desc'].str.contains(splitter)\n",
    "    workorders.loc[mask, 'workorder_desc'] = workorders.loc[mask, 'workorder_desc'].str.split(splitter, n=1).str[1].str.lower()\n",
    "    \n",
    "    return workorders[KEY_COLUMNS]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:57.662617Z",
     "start_time": "2023-04-20T21:30:57.633260Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptions = preprocess(workorders, \":>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:58.905471Z",
     "start_time": "2023-04-20T21:30:57.664540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('all-distilroberta-v1') #all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T21:30:58.914229Z",
     "start_time": "2023-04-20T21:30:58.907532Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def soft_cosine_similarity(v1, v2, similarity_matrix):\n",
    "    \"\"\"Calculate Soft Cosine Similarity between two vectors\"\"\"\n",
    "    return np.dot(v1, similarity_matrix).dot(v2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T22:20:41.469564Z",
     "start_time": "2023-04-20T22:20:41.464531Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_descriptions(df, location, date, description, window=5):\n",
    "    \"\"\"Get similar descriptions for given location, date, and description\"\"\"\n",
    "    \n",
    "    n_similar=5\n",
    "    \n",
    "    # Subset dataframe by location and date window\n",
    "    start_time = pd.to_datetime(date) - pd.DateOffset(hours=window)\n",
    "    end_time = pd.to_datetime(date) + pd.DateOffset(hours=window)\n",
    "    df_subset = df[(df['client_property_name'] == location) & (df['workorder_creation_date_ts'].between(start_time, end_time))]\n",
    "    \n",
    "    # Convert descriptions to list\n",
    "    embeddings = model.encode(df_subset['workorder_desc'].tolist())\n",
    "    description_embedding = model.encode(description)\n",
    "    \n",
    "    # compute soft cosine similarity matrix\n",
    "    sim_matrix = util.pytorch_cos_sim(description_embedding, embeddings)\n",
    "\n",
    "    return sim_matrix.numpy()[0], df_subset['workorder_desc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T22:24:12.075844Z",
     "start_time": "2023-04-20T22:23:43.865919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dupes: 4\n",
      "Location: Changi Business Park (L3) Zone 3D, Date: 2022-04-18 07:00:00\n",
      "Description: Hi,\n",
      "\n",
      "I've forgot the combination code for the locker L3-D-011. Please help in opening it.\n",
      "Duplicate Descriptions:\n",
      "Work order: Hi,\n",
      "\n",
      "I've forgot the combination code for the locker L3-D-011. Please help in opening it.\n",
      "Similarity Score: 0.9999999\n",
      "Work order: Hi,\n",
      "\n",
      "I've forgot the combination code for the locker L3-D-011. Please help in opening it.\n",
      "Similarity Score: 0.9999999\n",
      "Work order: Hi,\n",
      "\n",
      "I've forgot the combination code for the locker L3-D-011. Please help in opening it.\n",
      "Similarity Score: 0.9999999\n",
      "Work order: Hi,\n",
      "\n",
      "I've forgot the combination code for the locker L3-D-011. Please help in opening it.\n",
      "Similarity Score: 0.9999999\n",
      "Work order: Hi,\n",
      "\n",
      "I've forgot the combination code for the locker L3-D-011. Please help in opening it.\n",
      "Similarity Score: 0.9999999\n",
      "\n",
      "\n",
      "Num dupes: 4\n",
      "Location: Changi Business Park 2 - 2nd Floor - Zone C, Date: 2022-06-28 03:00:00\n",
      "Description: @C10 Level 2 monitor arm is loose. CH Michael\n",
      "Duplicate Descriptions:\n",
      "Work order: @C10 Level 2 monitor arm is loose. CH Michael\n",
      "Similarity Score: 1.0000005\n",
      "Work order: @C10 Level 2 monitor arm is loose. CH Michael\n",
      "Similarity Score: 1.0000005\n",
      "Work order: @C10 Level 2 monitor arm is loose. CH Michael\n",
      "Similarity Score: 1.0000005\n",
      "Work order: @C10 Level 2 monitor arm is loose. CH Michael\n",
      "Similarity Score: 1.0000005\n",
      "Work order: @C10 Level 2 monitor arm is loose. CH Michael\n",
      "Similarity Score: 1.0000005\n",
      "\n",
      "\n",
      "Num dupes: 1\n",
      "Location: RMZ Ecoworld, 6AB, 10th Floor Workstation area, Date: 2022-07-28 23:00:00\n",
      "Description: PA- Dusting required\n",
      "Duplicate Descriptions:\n",
      "Work order: PA- Dusting required\n",
      "Similarity Score: 1.0000001\n",
      "Work order: PA- Dusting required\n",
      "Similarity Score: 1.0000001\n",
      "\n",
      "\n",
      "Num dupes: 1\n",
      "Location: RMZ Ecoworld, 6B, 10th Floor Training room, Date: 2022-05-16 00:00:00\n",
      "Description: PA- Mat pickup required in training room-2\n",
      "Duplicate Descriptions:\n",
      "Work order: PA- Mat pickup required in training room-2\n",
      "Similarity Score: 0.9999995\n",
      "Work order: PA- Mat pickup required in training room-2\n",
      "Similarity Score: 0.9999995\n",
      "\n",
      "\n",
      "Num dupes: 1\n",
      "Location: RMZ Ecoworld, 6B, 9th Floor Workstation area, Date: 2022-05-19 23:00:00\n",
      "Description: PA- false ceiling need to be fixed properly near sprint 24\n",
      "Duplicate Descriptions:\n",
      "Work order: PA- false ceiling need to be fixed properly near sprint 24\n",
      "Similarity Score: 1.0000001\n",
      "Work order: PA- false ceiling need to be fixed properly near sprint 24\n",
      "Similarity Score: 1.0000001\n",
      "\n",
      "\n",
      "Num dupes: 2\n",
      "Location: RMZ Ecoworld, 6B, 9th Floor Workstation area, Date: 2022-05-24 23:00:00\n",
      "Description: PA-9TH floor B wing workstation area lights not working 2nos\n",
      "Duplicate Descriptions:\n",
      "Work order: PA-9TH floor B wing workstation area lights not working 2nos\n",
      "Similarity Score: 1.0000004\n",
      "Work order: PA-9TH floor B wing workstation area lights not working 2nos\n",
      "Similarity Score: 1.0000004\n",
      "Work order: PA-9TH floor B wing workstation area lights not working 2nos\n",
      "Similarity Score: 1.0000002\n",
      "\n",
      "\n",
      "Num dupes: 1\n",
      "Location: RMZ Ecoworld, 8A, 2nd Floor, Work Station Area, Date: 2022-09-23 00:00:00\n",
      "Description: sprint no 2E2 LIGHT NOT WORKING\n",
      "Duplicate Descriptions:\n",
      "Work order: sprint no 2E2 LIGHT NOT WORKING\n",
      "Similarity Score: 1.0000004\n",
      "Work order: sprint no 2E2 LIGHT NOT WORKING\n",
      "Similarity Score: 1.0000004\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped = descriptions.groupby(['client_property_name', pd.Grouper(key='workorder_creation_date_ts', freq='5H')])\n",
    "# Iterate over groups and get similar descriptions\n",
    "group_num = 0\n",
    "more_than_5 = 0\n",
    "total_number_dupes = 0\n",
    "for (location, date), group in grouped:\n",
    "    group_num+=1\n",
    "    if group.shape[0]>=5:\n",
    "        more_than_5+=1\n",
    "        description = group['workorder_desc'].iloc[0]\n",
    "        sim_mat, work_orders = get_similar_descriptions(descriptions, location, date, description)\n",
    "        sorted_similarities = (-sim_mat).argsort()[:5]\n",
    "        if len([cx for cx in sorted_similarities if sim_mat[cx] > 0.9999])>1:\n",
    "            print(\"Num dupes:\", len([cx for cx in sorted_similarities if sim_mat[cx] > 0.9999])-1)\n",
    "            print(f\"Location: {location}, Date: {date}\")\n",
    "            print(f\"Description: {description}\")\n",
    "            print(\"Duplicate Descriptions:\")\n",
    "            for pos in sorted_similarities:\n",
    "                if sim_mat[pos] > 0.9999:\n",
    "                    total_number_dupes+=1\n",
    "                    print(\"Work order:\", work_orders[pos])\n",
    "                    print(\"Similarity Score:\", sim_mat[pos])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T22:24:38.779341Z",
     "start_time": "2023-04-20T22:24:38.775239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_number_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
